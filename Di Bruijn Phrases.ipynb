{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "\n",
    "custom_stopwords = ['rt', '&amp', '#', '', '&amp;', '-', 'amp', '.', 'QQQQQQQQQ', 'll','re','ve']\n",
    "stopwords_english = stopwords.words('english')\n",
    "stopwords_spanish = stopwords.words('spanish')\n",
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "def is_word_blacklisted(word):\n",
    "    return word.startswith('http') or \\\n",
    "           word.startswith('//') or \\\n",
    "           word.startswith('@') or \\\n",
    "           word.startswith('#')# or \\\n",
    "        #   word in custom_stopwords or \\\n",
    "         #  word in stopwords_english or \\\n",
    "         #  word in stopwords_spanish\n",
    "\n",
    "def clean_text(text, remove_non_chars=True):\n",
    "    new_text = re.sub(r'@\\S+', '', text)\n",
    "    new_text = re.sub(r'#\\S+', '', new_text)\n",
    "    new_text = re.sub(r',', '.', new_text)\n",
    "    new_text = re.sub(r\"'\", ' ', new_text)\n",
    "    new_text = re.sub(r'https://\\S+', '', new_text)\n",
    "    new_text = re.sub(r'http://\\S+', '', new_text)\n",
    "    words = tknzr.tokenize(new_text)\n",
    "\n",
    "    if remove_non_chars:\n",
    "        words = [re.sub(r'\\W+', ' ', w) for w in words]\n",
    "\n",
    "    return ' '.join([w for w in words if not is_word_blacklisted(w)])\n",
    "\n",
    "def find_all_paths(G):\n",
    "    result = []\n",
    "    try:\n",
    "        result = []\n",
    "        start_nodes = [k for k,v in G.in_degree().iteritems() if v == 0]\n",
    "\n",
    "        for start in start_nodes:\n",
    "            paths = nx.shortest_path(G, start)\n",
    "            longest = max(paths, key= lambda x: len(set(paths[x])))\n",
    "            result.append(paths[longest])\n",
    "    except Exception as e:\n",
    "        logging.info(\"Error, \" + e.message)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_path_tuple(p, l):\n",
    "    d= dict(l)\n",
    "    prev = p[0]\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    min_cnt = 9999999\n",
    "    min_term = ''\n",
    "    phrase = p[0].split()[0]\n",
    "    for i in range(1, len(p)):\n",
    "        terms  = (prev + ' ' + p[i]).split()\n",
    "        term = terms[0] + ' ' + terms[1] + ' ' + terms[3]\n",
    "\n",
    "        if term in d:\n",
    "            cnt = d[term]\n",
    "\n",
    "            if cnt < min_cnt:\n",
    "                min_cnt = cnt\n",
    "                min_term = term\n",
    "\n",
    "            count+=1\n",
    "            sum += cnt\n",
    "\n",
    "        prev = p[i]\n",
    "        phrase += ' ' + p[i].split()[0]\n",
    "\n",
    "    phrase += ' ' +  p[-1].split()[1]\n",
    "    return phrase, min_cnt, min_term\n",
    "\n",
    "def build_phrases_from_aggs(aggs):\n",
    "    l = sorted(aggs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(l)):\n",
    "\n",
    "        bgram = l[i][0]\n",
    "        count = l[i][1]\n",
    "        terms = bgram.split()\n",
    "\n",
    "        term1 = terms[0] + ' ' + terms[1]\n",
    "        term2 = terms[1] + ' ' + terms[2]\n",
    "\n",
    "        G.add_edge(term1, term2, weight=count)\n",
    "\n",
    "    paths = find_all_paths(G)\n",
    "    phrases = [get_path_tuple(p,l) for p in paths]\n",
    "\n",
    "    keys = list(set([x[2] for x in phrases]))\n",
    "    d = [( k , [(x[0],x[1]) for x in phrases if x[2] == k]) for k in keys]\n",
    "\n",
    "    result = [{'phrase' : v[0][0], 'count' : v[0][1], 'alias' : k} for k,v in dict(d).iteritems()]\n",
    "\n",
    "\n",
    "    return sorted(result, key=lambda x : x['count'], reverse=True)\n",
    "\n",
    "\n",
    "def get_grams(text):\n",
    "    return [ ' '.join(g) for g in nltk_ngrams(text.split(),3)]\n",
    "\n",
    "def get_cluster_phrases(cluster_tweets):\n",
    "    cnt = Counter()\n",
    "    for t in cluster_tweets:\n",
    "        cnt.update(get_grams(t))\n",
    "    return build_phrases_from_aggs(cnt.most_common(int(np.sqrt(len(cluster_tweets)))))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/dima/Downloads/jan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data['Body_clean'] = data['Body']\n",
    "data = data[~data['Body'].str.contains('@OfficialJimRohn #Leadership #Success')]\n",
    "data = data[~data['Body'].str.startswith('RT @')]\n",
    "data['Body_clean'] = data['Body_clean'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instagram photo by higher perspective jun( 165)\n",
      "i just entered the competition to a bunch of goodies want to scupper my chances( 100)\n",
      "you don t( 65)\n",
      "like a boss girls( 65)\n",
      "www kaylaitsines com app( 61)\n",
      "when you know what you( 56)\n",
      "want it bad enough you( 56)\n",
      "you ll find a way to get it( 52)\n",
      "we get paid for( 52)\n",
      "only for you( 50)\n"
     ]
    }
   ],
   "source": [
    "phrases = get_cluster_phrases(data['Body_clean'].values)\n",
    "for p in phrases:\n",
    "    print p['phrase'] + '( '+ str(p['count']) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8289"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
