{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec \n",
    "model = Word2Vec.load_word2vec_format('word2vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "custom_stopwords = ['rt', '&amp', '#', '', '&amp;', '-', 'amp', '.', 'QQQQQQQQQ', 'll','re','ve']\n",
    "stopwords_english = stopwords.words('english')\n",
    "stopwords_spanish = stopwords.words('spanish')\n",
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def is_word_blacklisted(word):\n",
    "    return word.startswith('http') or \\\n",
    "           word.startswith('//') or \\\n",
    "           word.startswith('@') or \\\n",
    "           word.startswith('#') or \\\n",
    "           word in custom_stopwords or \\\n",
    "           word in stopwords_english or \\\n",
    "           word in stopwords_spanish\n",
    "\n",
    "def clean_text(text, remove_non_chars=True):\n",
    "    new_text = re.sub(r'@\\S+', '', text)\n",
    "    new_text = re.sub(r'#\\S+', '', new_text)\n",
    "    new_text = re.sub(r',', '.', new_text)\n",
    "    new_text = re.sub(r\"'\", ' ', new_text)\n",
    "    new_text = re.sub(r'https://\\S+', '', new_text)\n",
    "    new_text = re.sub(r'http://\\S+', '', new_text)\n",
    "    words = tknzr.tokenize(new_text)\n",
    "\n",
    "    if remove_non_chars:\n",
    "        words = [re.sub(r'\\W+', ' ', w) for w in words]\n",
    "\n",
    "    return ' '.join([w for w in words if not is_word_blacklisted(w)])\n",
    "\n",
    "def get_tweet_vector(text):\n",
    "    words = text.split()\n",
    "    N = len(words)\n",
    "    vector = np.zeros(300)\n",
    "    \n",
    "    if N > 0:\n",
    "        for word in words:\n",
    "            if word in model:\n",
    "                vector += model[word]\n",
    "        vector /= N\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/dima/Sweatergate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866\n"
     ]
    }
   ],
   "source": [
    "hashtags_cnt = Counter()\n",
    "hashtags_array = []\n",
    "tweets_with_hashtags = 0\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(df)):\n",
    "    text = df['Body'][i].lower()\n",
    "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
    "    hashtags_cnt.update(hashtags)\n",
    "    vector = get_tweet_vector(text)\n",
    "    \n",
    "    if len(hashtags) > 0: tweets_with_hashtags += 1\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        if hashtag not in hashtags_array:\n",
    "            i = len(hashtags_array)\n",
    "            hashtags_array.append(hashtag)\n",
    "        else:\n",
    "            i = hashtags_array.index(hashtag)\n",
    "            \n",
    "        X.append(vector)\n",
    "        y.append(y)\n",
    "    \n",
    "print tweets_with_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2268"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
